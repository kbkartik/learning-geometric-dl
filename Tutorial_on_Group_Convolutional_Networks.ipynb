{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWwwHcWOPqLe"
      },
      "source": [
        "# Group Equivariant Neural Networks\n",
        "\n",
        "---\n",
        "\n",
        "Notebook written by *Gabriele Cesa* (cesa.gabriele@gmail.com)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW5FOY2UWs-F"
      },
      "source": [
        "\n",
        "## Instructions\n",
        "\n",
        "Instructions:\n",
        "- Make a copy of this notebook and add your name to the title.\n",
        "- Fill in your code solutions between `### BEGIN SOLUTION` and `### END SOLUTION` and your answers to the open-ended questions where you see `YOUR ANSWER`. Do not create additional cells.\n",
        "- Your code should run without errors in Google Colab.\n",
        "- For you solution to be correct, you should achieve at least the task accuracy as stated in the tasks (although some unlucky seeds may give you slightly lower accuracy).\n",
        "- Any questions that may arise can be directed to your TA or asked on Campuswire.\n",
        "- This homework has a total of 100 points but includes an additional (harder) bonus question which can give you up to 15 additional points (for a total of 115 points).\n",
        "This question is not necessary to obtain a full grade for this assignment but can give you some additional points which are going to be taken into account in the final grade.\n",
        "- This assignment is due ***23:59PM GMT August 6, 2021***.\n",
        "- You should hand in a link to a Google Colab notebook containing your solutions.\n",
        "\n",
        "Good luck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hjxzT34dK-"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "During the lectures, you have learnt that the symmetries of a machine learning task can be modelled with **groups**.\n",
        "\n",
        "For example, an image classification task has a *rotational* symmetry if the label/class of an image does not depend on the orientation of the image, i.e. it is invariant to rotations of the image.\n",
        "This symmetry is modelled by the group $SO(2)$, i.e. the group of all planar rotations.\n",
        "\n",
        "When a task is symmetric with respect to a group $G$, it is usually beneficial to leverage this prior knowledge by designing our neural network to be **equivariant** to $G$.\n",
        "\n",
        "In this homework, you will first test your understanding of some concepts from *Group Theory*.\n",
        "Next, you will use these concepts to build a group-convolution layer and, therefore, a group-convolutional network.\n",
        "You will evaluate your models on the MNIST rot dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU3gfqICcuDO"
      },
      "source": [
        "Before starting, let's install a few useful packages.\n",
        "In this homework, we will mostly need **numpy** and **pytorch**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uWqGao5cz0_"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj_0_Ky8m9wj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc3AaBNaVXYX"
      },
      "source": [
        "# 1. Group Theory\n",
        "\n",
        "First of all, let's review some Group Theory.\n",
        "\n",
        "Recall the defintion of a **group**:\n",
        "\n",
        "\n",
        "\n",
        "> **Definition**: *Group* \\\\\n",
        " A group $(G, \\cdot)$ is a set $G$ together with a binary operation (the *group law*) $\\cdot : G \\times G \\to G$ which satisfies the following axioms:\n",
        "*   *Associativity*:  $\\forall a, b, c \\in G \\quad a \\cdot (b \\cdot c) = (a \\cdot b) \\cdot c$\n",
        "*   *Identity*:  $\\exists e \\in G : \\forall g \\in G \\quad g \\cdot e = e \\cdot g = g$\n",
        "*   *Inverse*:   $\\forall g \\in G \\ \\ \\exists g^{-1} \\in G: \\quad g g^{-1} = g^{-1} g = e$\n",
        "\n",
        "To simplify the notation, it is common to use $G$ also to refer to the group instead of just the set of its elements.\n",
        "\n",
        "Note that a group is *not* necessarily commutative, i.e. $a \\cdot b \\neq b \\cdot a$ generally.\n",
        "A group with commutative operations is called an **abelian** group.\n",
        "\n",
        "Useful property: $(a \\cdot b)^{-1} = b^{-1} \\cdot a^{-1}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63Uw490YaSO"
      },
      "source": [
        "### 1.1 Exercise: Prove Uniqueness of the Inverse     (5pt) (Pending)\n",
        "Given a group $G$, for any element $a \\in G$, there exists a unique element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$, where $e$ is the identity.\n",
        "We usually indicate the inverse of $a$ with $a^{-1}$.\n",
        "In this exercise, you will prove this property.\n",
        "\n",
        "To do so, show that if $a, b, c \\in G$ and $a \\cdot b = b \\cdot a = e$ and $a \\cdot c = c \\cdot a = e$, then $b = c$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnCSHQkMfqG-"
      },
      "source": [
        "#### 1.1 Insert Your Solution Here:\n",
        "\n",
        "> YOUR ANSWER\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-9lYnEYfXN2"
      },
      "source": [
        "### 1.2 Exercise: Implement the Group $C_4$ (5pt)\n",
        "\n",
        "Consider the group $G$ of rotations by multiples of $\\frac{\\pi}{2}$.\n",
        "This group contains $4$ elements and is called the **cyclic group** of order $4$; we usually indicate it with $G=C_4$:\n",
        "$$\n",
        "  C_4 = \\{R_{0}, R_{\\frac{\\pi}{2}}, R_{\\pi}, R_{3\\frac{\\pi}{2}} \\} = \\{ R_{r \\frac{\\pi}{2}} \\ |\\ r = 0, 1, 2, 3\\}\n",
        "$$\n",
        "Note that we can identify the elements of $C_4$ with $\\mathbb{Z} / 4\\mathbb{Z}$, i.e. the integers *modulo* $4$; indeed:\n",
        "$$\n",
        "  R_{r \\frac{\\pi}{2}} \\cdot R_{s \\frac{\\pi}{2}} = R_{(r + s \\mod 4) \\frac{\\pi}{2}}\n",
        "$$\n",
        "\n",
        "To simplify the notation, we will sometimes just write $r$ instead of $R_{r \\frac{\\pi}{2}}$.\n",
        "\n",
        "In this execise, you will implement the operations of the group $C_4$.\n",
        "Fill blocks in the following template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyI6SBKyjekU"
      },
      "outputs": [],
      "source": [
        "class C4:\n",
        "\n",
        "  @staticmethod\n",
        "  def product(r: int, s: int) -> int:\n",
        "    # Implements the *group law* of the group C_4.\n",
        "    # The input `r` and `s` must be integers in {0, 1, 2, 3} and represent two elements of the group.\n",
        "    # The method should return the integer representing the product of the two input elements.\n",
        "    # You should also check that the inputs are valid.\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    possible_rotations = (0, 1, 2, 3)\n",
        "\n",
        "    assert r in possible_rotations and s in possible_rotations, 'r or s not in range'\n",
        "\n",
        "    return np.remainder(r+s, 4)\n",
        "\n",
        "    ### END SOLUTION\n",
        "  \n",
        "  @staticmethod\n",
        "  def inverse(r: int) -> int:\n",
        "    # Implements the *inverse* operation of the group C_4.\n",
        "    # The input `r` must be an integer in {0, 1, 2, 3} and represents an element of the group.\n",
        "    # The method should return the integer representing the inverse of input element.\n",
        "    # You should also check that the input is valid.\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    possible_rotations = (0, 1, 2, 3)\n",
        "    assert r in possible_rotations, 'r not in range'\n",
        "    \n",
        "    return -1*np.remainder(r, 4)\n",
        "\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0RNEk59k-9M"
      },
      "outputs": [],
      "source": [
        "# Some test cases to check if your implementation is working\n",
        "assert C4.product(1, 3) == 0\n",
        "assert C4.product(0, 0) == 0\n",
        "assert C4.product(2, 3) == 1\n",
        "assert C4.inverse(0) == 0\n",
        "assert C4.inverse(1) == 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_rXVJ_62WQN"
      },
      "source": [
        "### 1.3 Exercise: Implement the Group $D_4$ (8pt)\n",
        "Let's now consider the more complex group of $4$ rotations *and* reflections.\n",
        "The **dihedral group** $D_4$ contains the $4$ rotations by multiples of $\\frac{\\pi}{2}$ and $4$ reflections along the $4$ different symmetry axes of a square:\n",
        "![reflections.png](data:image/png;base64,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)\n",
        "\n",
        "In total, $D_4$ contains $8$ elements:\n",
        "$$\n",
        "  D_4 = \\{R_{0}, R_{\\frac{\\pi}{2}}, R_{\\pi}, R_{3\\frac{\\pi}{2}}, F, R_{\\frac{\\pi}{2}} \\cdot F, R_{\\pi} \\cdot F,  R_{3\\frac{\\pi}{2}}  \\cdot F \\}\n",
        "$$\n",
        "The first four elements are the elements of $C_4$.\n",
        "The other four are obtained by first applying a reflection $F$ along the *horizontal axis* and then rotating by a multiple of $\\frac{\\pi}{2}$ .\n",
        "You can verify that all the four reflection in the image above can indeed be obtained this way.\n",
        "\n",
        "This means that we can identify an element $g \\in D_4$ of this group a pair $(f, r)$, where $r = 0, 1, 2, 3$ indicates the rotation while $f = 0, 1$ indicates whether we apply a reflection or not.\n",
        "\n",
        "In this execise, you will implement the operations of the group $D_4$.\n",
        "This time, however, you will also need to finish deriving the group law.\n",
        "To do so, you need to know:\n",
        "*   two reflections give an identity, i.e. $F \\cdot F = R_0$\n",
        "*   a reflection inverts a rotation, i.e. $F \\cdot R_{r \\frac{\\pi}{2}} = R_{r \\frac{\\pi}{2}}^{-1} \\cdot F$\n",
        "\n",
        "Now, using the other properties of the group law (recall the definition at the beginning of this notebook), you can write the product between any pair of elements $a, b$ of $D_4$.\n",
        "Once you have found the right group law, you will implement it in Python.\n",
        "Fill the blocks in the following template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlGnVQtEBkhI"
      },
      "outputs": [],
      "source": [
        "class D4:\n",
        "\n",
        "  @staticmethod\n",
        "  def product(a: tuple, b: tuple) -> tuple:\n",
        "    # Implements the *group law* of the group D_4.\n",
        "    # The input `a` and `b` must be tuples containing two integers, e.g. `a = (f, r)`.\n",
        "    # The two integeres indicate whether the group element includes a reflection and the number of rotations.\n",
        "    # The method should return the tuple representing the product of the two input elements.\n",
        "    # You should also check that the inputs are valid.\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    possible_rotations = (0, 1, 2, 3)\n",
        "    possible_reflections = (0, 1)\n",
        "    \n",
        "    f1, r1 = a\n",
        "    f2, r2 = b\n",
        "\n",
        "    assert r1 in possible_rotations and r2 in possible_rotations, 'rotations are not in range'\n",
        "    assert f1 in possible_reflections and f2 in possible_reflections, 'reflections are not in range'\n",
        "\n",
        "    inv_r1 = inverse(a)\n",
        "    inv_r2 = inverse(b)\n",
        "\n",
        "    return np.remainder(inv_r1+inv_r2, 4)\n",
        "\n",
        "    ### END SOLUTION\n",
        "  \n",
        "  @staticmethod\n",
        "  def inverse(g: int) -> int:\n",
        "    # Implements the *inverse* operation of the group D_4.\n",
        "    # The input `g` must be a tuple containing two integers, e.g. `g = (f, r)`.\n",
        "    # The two integeres indicate whether the group element includes a reflection and the number of rotations.\n",
        "    # The method should return the tuple representing the inverse of the input element.\n",
        "    # You should also check that the input is valid.\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    possible_rotations = (0, 1, 2, 3)\n",
        "    possible_reflections = (0, 1)\n",
        "    \n",
        "    f, r = g\n",
        "\n",
        "    assert f in possible_reflections, 'rotations are not in range'\n",
        "    assert r in possible_rotations, 'reflections are not in range'\n",
        "\n",
        "    return -1*np.remainder(r, 4)\n",
        "\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMelSLiME1fk"
      },
      "source": [
        "Let's check if the implementation works!\n",
        "\n",
        "You should play with these operations using other inputs and make sure the outputs you find match your expectations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj9Avgk3Dqq0"
      },
      "outputs": [],
      "source": [
        "e = (0, 0) # the identity element\n",
        "f = (1, 0) # the horizontal reflection\n",
        "r = (0, 1) # the rotation by 90 degrees\n",
        "\n",
        "# Let's verify that the implementation is consistent with the instructions given\n",
        "assert D4.product(e, e) == e\n",
        "assert D4.product(f, f) == e\n",
        "assert D4.product(f, r) == D4.product(D4.inverse(r), f)\n",
        "\n",
        "# Let's verify that the implementation satisfies the group axioms\n",
        "a = (1, 2)\n",
        "b = (0, 3)\n",
        "c = (1, 1)\n",
        "assert D4.product(a, e) == a\n",
        "assert D4.product(e, a) == a\n",
        "assert D4.product(b, D4.inverse(b)) == e\n",
        "assert D4.product(D4.inverse(b), b) == e\n",
        "\n",
        "assert D4.product(D4.product(a, b), c) == D4.product(a, D4.product(b, c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYxbur_TblVO"
      },
      "source": [
        "We now introduce a new defintion:\n",
        "\n",
        "> **Definition**: *Group Action* \\\\\n",
        "Let $G$ be a group. A **$G$-space** $X$ is a set $X$ equipped with a *group action* $. : G \\times X \\to X,\\ (g, x) \\mapsto g.x$ which satisfies the following axioms:\n",
        "*   *Identity*: $\\forall x \\in X \\quad e.x = x$ \n",
        "*   *Compatibility*: $\\forall a, b \\in G \\ \\forall x \\in X \\quad a.(b.x) = (a \\cdot b).x$ \n",
        "\n",
        "We say that $G$ *acts on* $X$.\n",
        "\n",
        "Let's see an example.\n",
        "Consider again the group $G=C_4$ of planar rotations by multiples of $\\frac{\\pi}{2}$.\n",
        "Let $X$ be the set of all $33 \\times 33$ gray-scale images.\n",
        "An image $x \\in X$ is interpreted as a function $x: \\mathbf{p} \\mapsto x[\\mathbf{p}] \\in \\mathbb{R}$ which maps each pixel (identified by its coordinates $\\mathbf{p} = (h,w)$) to a real number.\n",
        "\n",
        "An element $g = R_{r \\frac{\\pi}{2}} \\in G$ transforms an image $x \\in X$ into the image $g.x \\in X$ through a counter-clockwise rotation by $r \\frac{\\pi}{2}$ radians.\n",
        "The rotated image $g.x$ is defined on each pixel $\\mathbf{p}$ as:\n",
        "$$\n",
        " [g.x](\\mathbf{p}) := x(g^{-1}.\\mathbf{p})\n",
        "$$\n",
        "where $g^{-1}.\\mathbf{p}$ is the pixel in the original image $x$ which is moved to the position $\\mathbf{p}$ in the new image $g.x$.\n",
        "\n",
        "This action can be easily implemented with **PyTorch**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZAc4CRKpB8N"
      },
      "outputs": [],
      "source": [
        "def rotate(x: torch.Tensor, r: int) -> torch.Tensor:\n",
        "  # Method which implements the action of the group element `g` indexed by `r` on the input image `x`.\n",
        "  # The method returns the image `g.x`\n",
        "  \n",
        "  # note that we rotate the last 2 dimensions of the input, since we want to later use this method to rotate minibatches containing multiple images\n",
        "  return x.rot90(r, dims=(-2, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AZkDS-npug-"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(1, 1, 33, 33)**2\n",
        "\n",
        "r = 1\n",
        "gx = rotate(x, r)\n",
        "\n",
        "plt.imshow(x[0, 0].numpy())\n",
        "plt.title('Original Image $x$')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(gx[0, 0].numpy())\n",
        "plt.title('Rotated Image $g.x$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjozNZ_JB8I"
      },
      "source": [
        "# 2. Equivariant Convolution Layers\n",
        "\n",
        "Let's try now to build a rotation equivariant convolution layer.\n",
        "An equivariant layer is a function $\\psi: X \\to Y$ from an input $G$-space $X$ to an output $G$-space $Y$.\n",
        "We assume the input space is the space of images we defined before but we still need to choose an output space $Y$.\n",
        "As a first example, we choose $Y = X$, i.e. the output is still a space of grayscale images.\n",
        "\n",
        "Our equivariant layer will be a convolution with a $3 \\times 3$ filter.\n",
        "Note that the use of convolution guarantees the translation equivariance.\n",
        "Unfortunately, this is not sufficient for guaranteeing rotation equivariance.\n",
        "Let's verify this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZECphckKVaI"
      },
      "outputs": [],
      "source": [
        "filter3x3 = torch.randn(1, 1, 3, 3)\n",
        "\n",
        "plt.imshow(filter3x3[0, 0].numpy())\n",
        "plt.title('Filter')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "psi_x = torch.conv2d(x, filter3x3, bias=None, padding=1)\n",
        "psi_gx = torch.conv2d(gx, filter3x3, bias=None, padding=1)\n",
        "\n",
        "g_psi_x = rotate(psi_x, r)\n",
        "\n",
        "plt.imshow(g_psi_x[0, 0].numpy())\n",
        "plt.title('$g.\\psi(x)$')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(psi_gx[0, 0].numpy())\n",
        "plt.title('$\\psi(g.x)$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Zj6GDMLryu"
      },
      "source": [
        "Clearly, the two images do not look the same!\n",
        "\n",
        "To ensure that both outputs are the same, we need to constrain the convolutional filter to be symmetric to rotations, i.e. to be an isotropic filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMXDrWASM0s-"
      },
      "outputs": [],
      "source": [
        "filter3x3 = torch.empty((1, 1, 3, 3))\n",
        "# fill the central pixel\n",
        "filter3x3[0, 0, 1, 1] = np.random.randn()\n",
        "# fill the ring of radius 1 around it with a unique value\n",
        "mask = torch.ones(3, 3, dtype=torch.bool)\n",
        "mask[1, 1] = 0\n",
        "filter3x3[0, 0, mask] = np.random.randn()\n",
        "\n",
        "plt.imshow(filter3x3[0, 0].numpy())\n",
        "plt.title('Filter')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "psi_x = torch.conv2d(x, filter3x3, bias=None, padding=1)\n",
        "psi_gx = torch.conv2d(gx, filter3x3, bias=None, padding=1)\n",
        "\n",
        "g_psi_x = rotate(psi_x, r)\n",
        "\n",
        "plt.imshow(g_psi_x[0, 0].numpy())\n",
        "plt.title('$g.\\psi(x)$')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(psi_gx[0, 0].numpy())\n",
        "plt.title('$\\psi(g.x)$')\n",
        "plt.show()\n",
        "\n",
        "assert torch.allclose(psi_gx, g_psi_x, atol=1e-6, rtol=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wawi0GONK9h"
      },
      "source": [
        "The two outputs are now equivalent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSehS5WvNeXX"
      },
      "source": [
        "### 2.1 Exercise: Implement a Rotation Equivariant Convolution layer with Isotropic Filters (12 pt) (Pending)\n",
        "\n",
        "We use this idea as a building block for our neural network.\n",
        "In this exercise, you will implement a `torch.nn.Module` which performs convolution with a learnable set of isotropic filters.\n",
        "Fill the blocks in the following template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRyWqO2KSu1t"
      },
      "outputs": [],
      "source": [
        "class IsotropicConv2d(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels: int, out_channels: int, bias: bool = True):\n",
        "    \n",
        "    super(IsotropicConv2d, self).__init__()\n",
        "\n",
        "    self.kernel_size = 3\n",
        "    self.stride = 1\n",
        "    self.dilation = 1\n",
        "    self.padding = 1\n",
        "    \n",
        "    # In this block you need to create a tensor which stores the learnable weights\n",
        "    # Recall that each 3x3 filter has only `2` learnable parameters, one for the center and one for the ring around it.\n",
        "    # In total, there are `in_channels * out_channels` different filters.\n",
        "    # Remember to wrap the weights tensor into a `torch.nn.Parameter` and set `requires_grad = True`\n",
        "\n",
        "    # initialize the weights with some random values from a normal distribution with std = 1 / sqrt(out_channels * in_channels)\n",
        "\n",
        "    self.weight = None\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    \n",
        "    \n",
        "    ### END SOLUTION\n",
        "\n",
        "    if bias:\n",
        "      self.bias = torch.nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
        "    else:\n",
        "      self.bias = None\n",
        "  \n",
        "  def build_filter(self) ->torch.Tensor:\n",
        "    # using the tensor of learnable parameters, build the `out_channels x in_channels x 3 x 3` filter\n",
        "    \n",
        "    # Make sure that the tensor `filter3x3` is on the same device of `self.weight`\n",
        "    \n",
        "    filter3x3 = None\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return filter3x3\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    _filter = self.build_filter()\n",
        "\n",
        "    return  torch.conv2d(x, _filter,\n",
        "                            stride=self.stride,\n",
        "                            padding=self.padding,\n",
        "                            dilation=self.dilation,\n",
        "                            bias=self.bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hFFwtix6Ld6"
      },
      "outputs": [],
      "source": [
        "# Let's check if the layer is really equivariant\n",
        "\n",
        "in_channels = 5\n",
        "out_channels = 10\n",
        "batchsize = 6\n",
        "S = 33\n",
        "\n",
        "layer = IsotropicConv2d(in_channels=in_channels, out_channels=out_channels, bias=True)\n",
        "layer.eval()\n",
        "\n",
        "x = torch.randn(batchsize, in_channels, S, S)\n",
        "gx = rotate(x, 1)\n",
        "\n",
        "\n",
        "psi_x = layer(x)\n",
        "psi_gx = layer(gx)\n",
        "\n",
        "g_psi_x = rotate(psi_x, 1)\n",
        "\n",
        "assert psi_x.shape == g_psi_x.shape\n",
        "assert psi_x.shape == (batchsize, out_channels, S, S)\n",
        "\n",
        "# check the model is giving meaningful outputs\n",
        "assert not torch.allclose(psi_x, torch.zeros_like(psi_x), atol=1e-4, rtol=1e-4)\n",
        "\n",
        "# check equivariance\n",
        "assert torch.allclose(psi_gx, g_psi_x, atol=1e-6, rtol=1e-6)\n",
        "\n",
        "# check the model has the right number of parameters\n",
        "assert layer.weight.numel() == in_channels * out_channels * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XfVcU1_Neio"
      },
      "source": [
        "Unfortunately, isotropic filters are not very expressive.\n",
        "Instead, we would like to use more general, unconstrained filters.\n",
        "To do so, we need to rely on **group convolution**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGrVfvt0NeuS"
      },
      "source": [
        "\n",
        "Group convolution is a generalization of the convolution operation over a group.\n",
        "The key idea is that of transforming a filter with a larger set of transformations, rather than just translations.\n",
        "In this homework, the transformations we consider will include both translations and rotations.\n",
        "\n",
        "Note: we will use the word *convolution* to be consistent with the deep learning literature, even if we technically are using a *cross-correlation*.\n",
        "\n",
        "Before defining group convolution, we need to introduce another group.\n",
        "\n",
        "---\n",
        "\n",
        "### The rotations and translations group\n",
        "\n",
        "$p4$ is the *Wallpaper group*, i.e. the group of translations and rotations by multiples of $\\frac{\\pi}{2}$ of a grid.\n",
        "\n",
        "An element of this group is identified by a tuple $(h, w, r)$, where $h, w \\in \\mathbb{Z}$ are integer translations and $r \\in C_4$ is a rotation.\n",
        "To reduce the notation, we write $(\\mathbf{t}, r)$ with $\\mathbf{t} = (h, w)$, instead of $(h, w, r)$.\n",
        "\n",
        "The element $(\\mathbf{t}, r)$ acts on a point $\\mathbf{p} \\in \\mathbb{Z}^2$ on the grid by first rotating the point with $r \\in C_4$ and then translating it with $\\mathbf{t}$, i.e. $(\\mathbf{t}, r).\\mathbf{p} = \\mathbf{t} + r.\\mathbf{p}$.\n",
        "The composition of two elements $(\\mathbf{t}, r)$ and $(\\mathbf{p}, s)$ is $(\\mathbf{t}, r) \\cdot (\\mathbf{p}, s) = (\\mathbf{t} + r.\\mathbf{p}, r\\cdot s)$.\n",
        "Finally, the inverse of $(\\mathbf{t}, r)$ is $(r^{-1}.-\\mathbf{t}, r^{-1})$.\n",
        "You should try to verify on paper that these rules are consistent with the group axioms in the definition of group given above.\n",
        "\n",
        "---\n",
        "\n",
        "### Group Convolution\n",
        "\n",
        "Let $X$ be again the space of grayscale images on a grid.\n",
        "Let now $\\psi \\in X$ be a filter and $x \\in X$ an input image.\n",
        "The group convolution of $\\psi \\star x$ is defined as:\n",
        "$$\n",
        "  [\\psi \\star x](\\mathbf{t}, r) := \\sum_{\\mathbf{p} \\in \\mathbb{Z}^2} \\psi((\\mathbf{t}, r)^{-1} \\mathbf{p})\\ x(\\mathbf{p})\n",
        "                                 = \\sum_{\\mathbf{p} \\in \\mathbb{Z}^2} \\psi(r^{-1} (\\mathbf{p} - \\mathbf{t}))\\ x(\\mathbf{p})\n",
        "$$\n",
        "Note: the output of the convolution is not anymore a grayscale image in $X$.\n",
        "It is now a function over the group $p4$ we considered; indeed, we store an output value for each different transformation $(\\mathbf{t}, r)$ we applied to the filter $\\phi$.\n",
        "\n",
        "Let's call the space of all output signals $Y$.\n",
        "This is the space of all functions on the group $p4$, i.e. $Y$ is the space of all functions $y: p4 \\to \\mathbb{R}$.\n",
        "$Y$ is a $G$-space with a natural action of $p4$ on it:\n",
        "$$\n",
        "  [(\\mathbf{t}, r).y](\\mathbf{p}, s) := y((\\mathbf{t}, r)^{-1} \\cdot (\\mathbf{p}, s)) = y(r^{-1}(\\mathbf{p} - \\mathbf{t}), r^{-1}s)\n",
        "$$\n",
        "\n",
        "In other words, the use of a general filter $\\psi$ (i.e. non isotropic) in a group convolution maps the input space of images $X$ into a new larger space $Y$.\n",
        "This operation is sometimes also called a **lifting convolution** since it maps the space $X$ to the more complex space $Y$.\n",
        "Note that a function $y \\in Y$ can be implemented as a $4$-channels image, where the $i$-th channel ($i = 0, 1, 2, 3$) is defined as $y_i(\\mathbf{t}) = y(\\mathbf{t}, r=i) \\in \\mathbb{R}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2URqxYeGm0S"
      },
      "source": [
        "### 2.2 Implement the Group Action on the space of functions over $p4$   (5pt)\n",
        "\n",
        "In this exercise, you will need to implement the action of a rotation $r \\in C_4$ on a function $y \\in Y$.\n",
        "In other words, you need to implement a function which takes $y$ in input and returns $r.y$, which is defined at each pixel $\\mathbf{p}$ as\n",
        "$$\n",
        "  [r.y](\\mathbf{p}, s) := y(r^{-1} (\\mathbf{p}, s)) = y(r^{-1}\\mathbf{p}, r^{-1}s)\n",
        "$$\n",
        "\n",
        "HINT: you can reuse the function `rotate` defined earlier to rotate the pixels' location; then, you will need to permute the $4$ channels at each pixel depending on $r$.\n",
        "Fill the block in the following template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM2c66ZOHuur"
      },
      "outputs": [],
      "source": [
        "def rotate_p4(y: torch.Tensor, r: int) -> torch.Tensor:\n",
        "  # `y` is a function over p4, i.e. over the pixel positions and over the elements of the group C_4.\n",
        "  # This method implements the action of a rotation `r` on `y`.\n",
        "  # To be able to reuse this function later with a minibatch of inputs, assume that the last two dimensions (`dim=-2` and `dim=-1`) of `y` are the spatial dimensions\n",
        "  # while `dim=-3` has size `4` and is the C_4 dimension.\n",
        "  # All other dimensions are considered batch dimensions\n",
        "  assert len(y.shape) >= 3\n",
        "  assert y.shape[-3] == 4\n",
        "\n",
        "  possible_rotations = (0, 1, 2, 3)\n",
        "  assert r in possible_rotations, 'rotations are not in range'\n",
        "\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "  y = rotate(y, r)\n",
        "\n",
        "  y = torch.roll(y, r, dims=-3)\n",
        "  return y\n",
        "  \n",
        "  ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIVqYFF3JUFq"
      },
      "outputs": [],
      "source": [
        "# Let's test a rotation by r=1\n",
        "\n",
        "y = torch.randn(1, 1, 4, 33, 33)**2\n",
        "\n",
        "\n",
        "ry = rotate_p4(y, 1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, sharex=True, sharey=True, squeeze=True, figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  axes[i].imshow(y[0, 0, i].numpy())\n",
        "fig.suptitle('Original y')\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, sharex=True, sharey=True, squeeze=True, figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  axes[i].imshow(ry[0, 0, i].numpy())\n",
        "fig.suptitle('Rotated y')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# check that the images are actually rotated:\n",
        "for _ in range(10):\n",
        "  p = np.random.randint(0, 33, size=2)\n",
        "  s = np.random.randint(0, 4)\n",
        "\n",
        "  # compute r^-1 s\n",
        "  _rs = C4.product(C4.inverse(1), s)\n",
        "  \n",
        "  # compute r^-1 p\n",
        "  # note that the rotation is around the central pixel (16, 16)\n",
        "  # A rotation by r^-1 = -90 degrees maps (X, Y) -> (Y, -X)\n",
        "  center = np.array([16, 16])\n",
        "  # center the point\n",
        "  centered_p = p - center\n",
        "  # rotate round the center\n",
        "  rotated_p = np.array([centered_p[1], -centered_p[0]])\n",
        "  # shift the point back\n",
        "  _rp = rotated_p + center\n",
        "\n",
        "  # Finally check that [r.y](p, s) = y(r^-1 p, r^-1 s)\n",
        "\n",
        "  # However, in a machine, an image is stored with the coordinates (H-1-Y, X) rather than the usual (X, Y), where H is the height of the image;\n",
        "  # we need to take this into account\n",
        "  assert torch.isclose(\n",
        "      ry[..., s, 32-p[1], p[0]],\n",
        "      y[..., _rs, 32-_rp[1], _rp[0]],\n",
        "      atol=1e-5, rtol=1e-5\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10XH6LQjwani"
      },
      "source": [
        "### 2.3 Implement Lifting Convolution (15 pt)\n",
        "\n",
        "In this exercise, you will build a `PyTorch` module which implements the *lifting convolution* described earlier.\n",
        "The input of this layer is a grayscale image $x \\in X$ (potentially a batch and with more channels).\n",
        "The output is a function $y \\in Y$ (again, potentially many of them by using multiple channels and mini-batches).\n",
        "For each input channel and output channel, the layer should perform a *lifting convolution* with a learnable filter.\n",
        "\n",
        "This can be realized by exploiting the usual `torch.conv2d` but by using $4$ rotated copies of a single learnable filter.\n",
        "To clarify this, observe the image below.\n",
        "The image shows an input image (containing a green lizard) and $4$ rotated copies of a filter (the small discs).\n",
        "The image is convolved with each copy independently to generate $4$ different output images, one for each rotation.\n",
        "Therefore, we can implement this operation by stacking the $4$ copies into a unique filter with $4$ output channels and using this new filter in `torch.conv2d`.\n",
        "\n",
        "The image also shows that a rotation of the input image results in each of the $4$ output images to rotate independently but also in the cyclic permutation of the $4$ images (look at the highlighted point in the top right image and where it moves to in the second row). \n",
        "\n",
        "**Lifting convolutions image**\n",
        "\n",
        "Finally, a convolutional layer usually includes a bias term.\n",
        "In a normal convolutional network, it is common to share the same bias over all pixels, i.e. the same bias is summed to the features at each pixel.\n",
        "Similarly, when we use a lifting convolution, we share the bias over all pixels but *also* over all rotations, i.e. over the $4$ output channels.\n",
        "\n",
        "\n",
        "\n",
        "Implement a *lifting convolution* by filling the blocks in the following template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M97wYeO5529v"
      },
      "outputs": [],
      "source": [
        "class LiftingConv2d(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int = 0, bias: bool = True):\n",
        "    \n",
        "    super(LiftingConv2d, self).__init__()\n",
        "\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = 1\n",
        "    self.dilation = 1\n",
        "    self.padding = padding\n",
        "    self.out_channels = out_channels\n",
        "    self.in_channels = in_channels\n",
        "    \n",
        "    # In this block you need to create a tensor which stores the learnable filters\n",
        "    # Recall that this layer should have `out_channels x in_channels` different learnable filters, each of shape `kernel_size x kernel_size`\n",
        "    # During the forward pass, you will build the bigger filter of shape `out_channels x 4 x in_channels x kernel_size x kernel_size` by rotating 4 times \n",
        "    # the learnable filters in `self.weight`\n",
        "    \n",
        "    # initialize the weights with some random values from a normal distribution with std = 1 / sqrt(out_channels * in_channels)\n",
        "\n",
        "    self.weight = None\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    self.weight = torch.empty((out_channels, in_channels, kernel_size, kernel_size), requires_grad=True, device=device)\n",
        "    torch.nn.init.normal_(self.weight, mean=0, std = 1/torch.sqrt(out_channels * in_channels))\n",
        "    \n",
        "    ### END SOLUTION\n",
        "\n",
        "    # This time, you also need to build the bias\n",
        "    # The bias is shared over the 4 rotations\n",
        "    # In total, the bias has `out_channels` learnable parameters, one for each independent output\n",
        "    # In the forward pass, you need to convert this bias into an \"expanded\" bias by repeating each entry `4` times\n",
        "    \n",
        "    self.bias = None\n",
        "    if bias:\n",
        "    ### BEGIN SOLUTION\n",
        "      self.bias = torch.empty(out_channels, requires_grad=True, device=device)\n",
        "      torch.nn.init.normal_(self.bias, mean=0, std = 1/torch.sqrt(out_channels))\n",
        "\n",
        "    ### END SOLUTION  \n",
        "  \n",
        "  def build_filter(self) ->torch.Tensor:\n",
        "    # using the tensors of learnable parameters, build \n",
        "    # - the `out_channels x 4 x in_channels x kernel_size x kernel_size` filter\n",
        "    # - the `out_channels x 4` bias\n",
        "    \n",
        "    _filter = None\n",
        "    _bias = None\n",
        "\n",
        "    # Make sure that the filter and the bias tensors are on the same device of `self.weight` and `self.bias`\n",
        "\n",
        "    # First build the filter\n",
        "    # Recall that `_filter[:, i, :, :, :]` should contain the learnable filter rotated `i` times\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    _filter = self.weight.clone().detach()\n",
        "    _filter = _filter.expand(self.out_channels, 4, self.in_channels, self.kernel_size, self.kernel_size)\n",
        "    \n",
        "    for i in [1, 2, 3]:\n",
        "      _filter[:, i, :, :, :] = rotate(_filter[:, i, :, :, :], i)\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    # Now build the bias\n",
        "    # Recall that `_bias[:, i]` should contain a copy of the learnable bias for each `i=0,1,2,3`\n",
        "\n",
        "    if self.bias is not None:\n",
        "    ### BEGIN SOLUTION\n",
        "      _bias = self.bias.clone().detach()\n",
        "      _bias = _bias.expand(self.out_channels, 4)\n",
        "\n",
        "    ### END SOLUTION\n",
        "    else:\n",
        "      _bias = None\n",
        "\n",
        "    return _filter, _bias\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    _filter, _bias = self.build_filter()\n",
        "\n",
        "    assert _bias.shape == (self.out_channels, 4)\n",
        "    assert _filter.shape == (self.out_channels, 4, self.in_channels, self.kernel_size, self.kernel_size)\n",
        "\n",
        "    # to be able to use torch.conv2d, we need to reshape the filter and bias to stack together all filters\n",
        "    _filter = _filter.reshape(self.out_channels * 4, self.in_channels, self.kernel_size, self.kernel_size)\n",
        "    _bias = _bias.reshape(self.out_channels * 4)\n",
        "\n",
        "    out = torch.conv2d(x, _filter,\n",
        "                       stride=self.stride,\n",
        "                       padding=self.padding,\n",
        "                       dilation=self.dilation,\n",
        "                       bias=_bias)\n",
        "    \n",
        "    # `out` has now shape `batch_size x out_channels*4 x W x H`\n",
        "    # we need to reshape it to `batch_size x out_channels x 4 x W x H` to have the shape we expect\n",
        "\n",
        "    return out.view(-1, self.out_channels, 4, out.shape[-2], out.shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rut1oTksa_t"
      },
      "outputs": [],
      "source": [
        "# Let's check if the layer is really equivariant\n",
        "\n",
        "in_channels = 5\n",
        "out_channels = 10\n",
        "kernel_size = 3\n",
        "batchsize = 6\n",
        "S = 33\n",
        "\n",
        "layer = LiftingConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size = kernel_size, padding=1, bias=True)\n",
        "layer.eval()\n",
        "\n",
        "x = torch.randn(batchsize, in_channels, S, S)\n",
        "# the input image belongs to the space X, so we use the original action to rotate it\n",
        "gx = rotate(x, 1)\n",
        "\n",
        "# compute the output\n",
        "psi_x = layer(x)\n",
        "psi_gx = layer(gx)\n",
        "\n",
        "# the output is a function in the space Y, so we need to use the new action to rotate it\n",
        "g_psi_x = rotate_p4(psi_x, 1)\n",
        "\n",
        "assert psi_x.shape == g_psi_x.shape\n",
        "assert psi_x.shape == (batchsize, out_channels, 4, S, S)\n",
        "\n",
        "# check the model is giving meaningful outputs\n",
        "assert not torch.allclose(psi_x, torch.zeros_like(psi_x), atol=1e-4, rtol=1e-4)\n",
        "\n",
        "# check equivariance\n",
        "assert torch.allclose(psi_gx, g_psi_x, atol=1e-6, rtol=1e-6)\n",
        "\n",
        "# check the model has the right number of parameters\n",
        "assert layer.weight.numel() == in_channels * out_channels * kernel_size**2\n",
        "assert layer.bias.numel() == out_channels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We just built a rotation equivariant convolutional layer which uses general and expressive filters!\n",
        "\n",
        "Unfortunately, we are not ready to build a deep convolutional network yet.\n",
        "Indeed, while the *lifting layer* is equivariant, its output space $Y$ is different from its input space $X$.\n",
        "This means we cannot directly feed the output of the lifting layer into another lifting layer to build a deep architecture...\n",
        "\n",
        "To solve this problem, recall the definition of **group convolution**.\n",
        "If $X$ is space of grayscale images on a grid and $\\psi, x \\in X$ are respectively the filter and the input image, the group convolution of $\\psi \\star x$ was defined as:\n",
        "$$\n",
        "  [\\psi \\star x](\\mathbf{t}, r) := \\sum_{\\mathbf{p} \\in \\mathbb{Z}^2} \\psi((\\mathbf{t}, r)^{-1} \\mathbf{p})\\ x(\\mathbf{p})\n",
        "$$\n",
        "Note that in this case, both the input $x$ and the filter $\\psi$ belong to $X$.\n",
        "This definition can be adapted to inputs (and therefore filters) which are functions over the elements of the group, i.e. functions in $Y$.\n",
        "\n",
        "Indeed, a group $G$ has a natural action on the functions over its elements; if $x: G \\to \\mathbb{R}$ and $g \\in G$, the function $g.x$ is defined as:\n",
        "$$\n",
        "  [g.x](h) := x(g^{-1} \\cdot h)\n",
        "$$\n",
        "where $g, h \\in G$.\n",
        "\n",
        "We can use this to build a group convolution with a filter $\\psi \\in Y$ instead of in $X$.\n",
        "Let $x, \\psi \\in Y$ be the input and the filter.\n",
        "Then $y = \\psi \\star x$ is defined as:\n",
        "$$\n",
        "  [\\psi \\star x](g) := \\sum_{h \\in H} \\psi(g^{-1} \\cdot h)\\ x(h)\n",
        "$$\n",
        "Note that the output $y =\\psi \\star x$ still belongs to $Y$, the space of functions over the group $G$.\n",
        "This means that the group convolution maps $Y$ to $Y$, so we can stack multiple of them to obtain a deep model!\n",
        "\n",
        "In the case of $G = p4$, the group of translations and rotations, this equation becomes:\n",
        "$$\n",
        "  [\\psi \\star x](\\mathbf{t}, r) := \\sum_{s \\in C_4} \\sum_{\\mathbf{p} \\in \\mathbb{Z}^2} \\psi((\\mathbf{t}, r)^{-1} (\\mathbf{p}, s))\\ x(\\mathbf{p}, s)\n",
        "$$\n",
        "By using the fact that $\\psi((\\mathbf{t}, r)^{-1} (\\mathbf{p}, s)) = \\psi(r^{-1} (\\mathbf{p} - \\mathbf{t}, s)) = [r.\\psi](\\mathbf{p} - \\mathbf{t}, s)$\n",
        "the equation becomes:\n",
        "$$\n",
        "  [\\psi \\star x](\\mathbf{t}, r) := \\sum_{s \\in C_4} \\sum_{\\mathbf{p} \\in \\mathbb{Z}^2} [r.\\psi](\\mathbf{p} - \\mathbf{t}, s)\\ x(\\mathbf{p}, s)\n",
        "$$\n",
        "\n",
        "Note that the term $\\sum_{\\mathbf{p} \\in \\mathbb{Z}^2} [r.\\psi](\\mathbf{p} - \\mathbf{t}, s)\\ x(\\mathbf{p}, s)$ is the usual convolution (`torch.conv2d`) of the grayscale image $x(\\cdot, s): \\mathbb{Z}^2 \\to \\mathbb{R}$ with the filter $[r.\\psi](\\cdot, s): \\mathbb{Z}^2 \\to \\mathbb{R}$, for a fixed value of $s$.\n",
        "Indeed, the group convolution above can be implemented by using $4 \\times 4$  `torch.conv2d` with normal filters.\n",
        "\n",
        "Look at the following image:\n",
        "\n",
        "**group-convolutions**\n",
        "\n",
        "The four images on the right represent the $4$ channels of the input.\n",
        "Since this is the output of a *lifting layer*, it contains a channel for each rotation $s$.\n",
        "The $4$ images on the left are the $4$ channels of the output.\n",
        "Again, there is channel for each rotation $r$.\n",
        "The $4 \\times 4$ grid of images represents the $16$ convolutions.\n",
        "The filter $\\phi \\in Y$ also has $4$ channels, one per rotation $s$; these $4$ channels are visualized in the first row of the grid.\n",
        "For simplicity we used the shorter notation $x_s = x(\\cdot, s)$, $\\psi_s = \\psi(\\cdot, s)$ and $y_r = y(\\cdot, r)$.\n",
        "\n",
        "Each other row of the grid contains a copy of the filter $\\psi$ (i.e. the first row) rotated by a rotation $r \\in C_4$.\n",
        "A rotation of $\\psi$ includes both a rotation of its pixels and a cyclic permutation of the $4$ channels.\n",
        "In particular, the image highlights the convolution for $r=3$ and $s=1$; the scalar filter $[r.\\psi](\\cdot, s=1)$ is the highlighted filter $r.\\psi_{s=2}$, since it is a rotated copy of $\\psi_{s=2} = \\psi(\\cdot, s=2)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwG6f5XM6R4h"
      },
      "source": [
        "### 2.4 Implement A Group-Convolution layer (15 pt)\n",
        "Now, you should implement this group convolution in a `torch.nn.Module`.\n",
        "For efficiency, you should rely on `torch.conv2d` as ealier.\n",
        "Fill the blocks in the following template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyNG6xYvDxRE"
      },
      "outputs": [],
      "source": [
        "class GroupConv2d(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int = 0, bias: bool = True):\n",
        "    \n",
        "    super(GroupConv2d, self).__init__()\n",
        "\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = 1\n",
        "    self.dilation = 1\n",
        "    self.padding = padding\n",
        "    self.out_channels = out_channels\n",
        "    self.in_channels = in_channels\n",
        "    \n",
        "    # In this block you need to create a tensor which stores the learnable filters\n",
        "    # Recall that this layer should have `out_channels x in_channels` different learnable filters, each of shape `4 x kernel_size x kernel_size`\n",
        "    # During the forward pass, you will build the bigger filter of shape `out_channels x 4 x in_channels x 4 x kernel_size x kernel_size` by rotating 4 times \n",
        "    # the learnable filters in `self.weight`\n",
        "    \n",
        "    # initialize the weights with some random values from a normal distribution with std = 1 / np.sqrt(out_channels * in_channels)\n",
        "\n",
        "    self.weight = None\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    self.weight = torch.empty((out_channels, in_channels, 4, kernel_size, kernel_size), requires_grad=True, device=device)\n",
        "    torch.nn.init.normal_(self.weight, mean=0, std = 1/np.sqrt(out_channels * in_channels))\n",
        "    \n",
        "    ### END SOLUTION\n",
        "\n",
        "    # The bias is shared over the 4 rotations\n",
        "    # In total, the bias has `out_channels` learnable parameters, one for each independent output\n",
        "    # In the forward pass, you need to convert this bias into an \"expanded\" bias by repeating each entry `4` times\n",
        "    \n",
        "    self.bias = None\n",
        "    if bias:\n",
        "    ### BEGIN SOLUTION\n",
        "      self.bias = torch.zeros((out_channels, 1), requires_grad=True, device=device)\n",
        "\n",
        "    ### END SOLUTION  \n",
        "  \n",
        "  def build_filter(self) ->torch.Tensor:\n",
        "    # using the tensors of learnable parameters, build \n",
        "    # - the `out_channels x 4 x in_channels x 4 x kernel_size x kernel_size` filter\n",
        "    # - the `out_channels x 4` bias\n",
        "    \n",
        "    _filter = None\n",
        "    _bias = None\n",
        "\n",
        "    # Make sure that the filter and the bias tensors are on the same device of `self.weight` and `self.bias`\n",
        "\n",
        "    # First build the filter\n",
        "    # Recall that `_filter[:, r, :, :, :, :]` should contain the learnable filter rotated `r` times\n",
        "    # Also, recall that a rotation includes both a rotation of the pixels and a cyclic permutation of the 4 rotational input channels\n",
        "\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    _filter = self.weight.clone().detach()\n",
        "    _filter = _filter.expand(self.out_channels, 4, self.in_channels, 4, self.kernel_size, self.kernel_size)\n",
        "\n",
        "    for i in [1, 2, 3]:\n",
        "      _filter[:, i, :, :, :, :] = rotate_p4(_filter[:, i, :, :, :, :], i)\n",
        "    \n",
        "    ### END SOLUTION\n",
        "\n",
        "    # Now build the bias\n",
        "    # Recall that `_bias[:, i]` should contain a copy of the learnable bias for each `i`\n",
        "\n",
        "    if self.bias is not None:\n",
        "    ### BEGIN SOLUTION\n",
        "      _bias = self.bias.clone().detach()\n",
        "      _bias = _bias.expand(self.out_channels, 4)\n",
        "\n",
        "    ### END SOLUTION\n",
        "    else:\n",
        "      _bias = None\n",
        "\n",
        "    return _filter, _bias\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    _filter, _bias = self.build_filter()\n",
        "\n",
        "    assert _bias.shape == (self.out_channels, 4)\n",
        "    assert _filter.shape == (self.out_channels, 4, self.in_channels, 4, self.kernel_size, self.kernel_size)\n",
        "\n",
        "    # to be able to use torch.conv2d, we need to reshape the filter and bias to stack together all filters\n",
        "    _filter = _filter.reshape(self.out_channels * 4, self.in_channels * 4, self.kernel_size, self.kernel_size)\n",
        "    _bias = _bias.reshape(self.out_channels * 4)\n",
        "\n",
        "    # this time, also the input has shape `batch_size x in_channels x 4 x W x H`\n",
        "    # so we need to reshape it to `batch_size x in_channels*4 x W x H` to be able to use torch.conv2d\n",
        "    x = x.view(x.shape[0], self.in_channels*4, x.shape[-2], x.shape[-1])\n",
        "\n",
        "    out = torch.conv2d(x, _filter,\n",
        "                       stride=self.stride,\n",
        "                       padding=self.padding,\n",
        "                       dilation=self.dilation,\n",
        "                       bias=_bias)\n",
        "    \n",
        "    # `out` has now shape `batch_size x out_channels*4 x W x H`\n",
        "    # we need to reshape it to `batch_size x out_channels x 4 x W x H` to have the shape we expect\n",
        "\n",
        "    return out.view(-1, self.out_channels, 4, out.shape[-2], out.shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGVKWbPb3Npq"
      },
      "outputs": [],
      "source": [
        "# Let's check if the layer is really equivariant\n",
        "\n",
        "in_channels = 5\n",
        "out_channels = 10\n",
        "kernel_size = 3\n",
        "batchsize = 4\n",
        "S = 33\n",
        "\n",
        "layer = GroupConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size = kernel_size, padding=1, bias=True)\n",
        "layer.eval()\n",
        "\n",
        "x = torch.randn(batchsize, in_channels, 4, S, S)**2\n",
        "# the input image belongs to the space Y, so this time we use the new action to rotate it\n",
        "gx = rotate_p4(x, 1)\n",
        "\n",
        "# compute the output\n",
        "psi_x = layer(x)\n",
        "psi_gx = layer(gx)\n",
        "\n",
        "# the output is a function in the space Y, so we need to use the new action to rotate it\n",
        "g_psi_x = rotate_p4(psi_x, 1)\n",
        "\n",
        "assert psi_x.shape == g_psi_x.shape\n",
        "assert psi_x.shape == (batchsize, out_channels, 4, S, S)\n",
        "\n",
        "# check the model is giving meaningful outputs\n",
        "assert not torch.allclose(psi_x, torch.zeros_like(psi_x), atol=1e-4, rtol=1e-4)\n",
        "\n",
        "# check equivariance\n",
        "assert torch.allclose(psi_gx, g_psi_x, atol=1e-5, rtol=1e-5)\n",
        "\n",
        "# check the model has the right number of parameters\n",
        "assert layer.weight.numel() == in_channels * out_channels * 4* kernel_size**2\n",
        "assert layer.bias.numel() == out_channels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0LJwqB07397"
      },
      "source": [
        "# 3. Equivariant Networks\n",
        "\n",
        "We are finally ready to build our equivariant neural network.\n",
        "An equivariant neural network starts with a *lifting layer* to map an input image $x \\in X$ into a function $y \\in Y$ over the group $p4$.\n",
        "We can now alternate a sequence of non-linearities (e.g. ReLU) and *group-convolutions*, which map $Y$ to $Y$.\n",
        "\n",
        "Since we will apply this network to the task of image classification, in the last layer we apply a *pooling* operation as is normally done in a CNN; this time, however, we also pool over the $4$ rotational channels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1wq7oC55nlr"
      },
      "source": [
        "### 3.1 Exercise. (5pt)\n",
        "\n",
        "Explain why we need to pool over the $4$ rotational channels.\n",
        "Why can't we keep the $4$ channels and use all of them as features of the final classifier?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7K5tTgn9iUc"
      },
      "source": [
        "#### 3.1 Insert Your Solution Here:\n",
        "\n",
        "> Pooling makes the network rotation invariant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZcmDFGU--j5"
      },
      "source": [
        "### 3.2 Implement A Deep Rotation Equivariant CNN (10 pt)\n",
        "\n",
        "Fianlly, you can combine the layers you have implemented earlier to build a rotation equivariant CNN.\n",
        "You model will take in input batches of $33 \\times 33$ images with a single input channel.\n",
        "\n",
        "The network performs a first *lifting layer* with $8$ output channels and is followed by $4$ *group convolution* with, respectively, $16$, $32$, $64$ and $128$ output channels.\n",
        "All convolutions have kernel size $3$, padding $1$ and stride $1$ and should use the bias.\n",
        "All convolutions are followed by `torch.nn.MaxPool3d` and `torch.nn.ReLU`.\n",
        "Note that we use `MaxPool3d` rather than `MaxPool2d` since our feature tensors have $5$ dimensions (there is an additional dimension of size $4$).\n",
        "In all pooling layers, we will use a kernel of size $(1, 3, 3)$, a stride of $(1, 2, 2)$ and a padding of $(0, 1, 1)$.\n",
        "This ensures pooling is done only on the spatial dimensions, while the rotational dimension is preserved.\n",
        "The last pooling layer, however, will also pool over the rotational dimension so it will use a kernel of size $(4, 3, 3)$, stride $(1, 1, 1)$ and padding $(0, 0, 0)$.\n",
        "\n",
        "Finally, the features extracted from the convolutional network are used in a linear layer to classify the input in $10$ classes.\n",
        "You don't need to apply a softmax layer on top.\n",
        "\n",
        "Follow the given template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocEWdJ3s58dc"
      },
      "outputs": [],
      "source": [
        "class C4CNN(torch.nn.Module):\n",
        "  def __init__(self, n_classes=10):\n",
        "\n",
        "    super(C4CNN, self).__init__()\n",
        "\n",
        "    channels = [8, 16, 32, 64, 128]\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    layers = []\n",
        "    for i, ch in zip(enumerate(channels)):\n",
        "      if ch == 8:\n",
        "        layers.append(torch.nn.LiftingConv2d(1, ch, 3, 1))\n",
        "      else:\n",
        "        layers.append(torch.nn.GroupConv2d(channels[i-1], ch, 3, 1))\n",
        "      if ch != 128:\n",
        "        layers.append(torch.nn.MaxPool3d((1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)))\n",
        "      else:\n",
        "        layers.append(torch.nn.MaxPool3d((4, 3, 3), stride=(1, 1, 1), padding=(0, 0, 0)))\n",
        "      layers.append(torch.nn.ReLU())\n",
        "\n",
        "    self.NN = torch.nn.Sequential(*layers)\n",
        "\n",
        "    self.fc_out = torch.nn.Linear(128, n_classes, device=device)\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    \n",
        "    ### BEGIN SOLUTION\n",
        "    output = self.NN(x)\n",
        "    pred = self.fc_out(output)\n",
        "    \n",
        "    return pred\n",
        "\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7CJu_wdDxmD"
      },
      "outputs": [],
      "source": [
        "# Let's try our model\n",
        "\n",
        "net = C4CNN()\n",
        "\n",
        "x = torch.randn(5, 1, 33, 33)\n",
        "\n",
        "y = net(x)\n",
        "\n",
        "assert y.shape == (5, 10)\n",
        "\n",
        "# Let's check if the model is invariant!\n",
        "\n",
        "gx = rotate(x, 1)\n",
        "\n",
        "gy = net(gx)\n",
        "\n",
        "assert torch.allclose(y, gy, atol=1e-5, rtol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61UFHuGcFBJi"
      },
      "source": [
        "Let's try now to train out model.\n",
        "We will train the network on rotated MNIST.\n",
        "\n",
        "First of all, we need to download the dataset.\n",
        "\n",
        "Then, we will provide a dataloader for the dataset and a training script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMa9RaivEU8b"
      },
      "outputs": [],
      "source": [
        "# download the dataset\n",
        "!wget -nc http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip\n",
        "# uncompress the zip file\n",
        "!unzip -n mnist_rotation_new.zip -d mnist_rotation_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82WKqC3jFWqJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "class MnistRotDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, mode, transform=None):\n",
        "        assert mode in ['train', 'test']\n",
        "            \n",
        "        if mode == \"train\":\n",
        "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_train_valid.amat\"\n",
        "        else:\n",
        "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_test.amat\"\n",
        "        \n",
        "        self.transform = transform\n",
        "\n",
        "        data = np.loadtxt(file, delimiter=' ')\n",
        "        \n",
        "        self.labels = data[:, -1].astype(np.int64)\n",
        "        self.num_samples = len(self.labels)    \n",
        "        self.images = data[:, :-1].reshape(-1, 28, 28).astype(np.float32)\n",
        "\n",
        "        # images in MNIST are only 28x28\n",
        "        # we pad them to have shape 33 x 33\n",
        "        self.images = np.pad(self.images, pad_width=((0,0), (2, 3), (2, 3)), mode='edge')\n",
        "\n",
        "        assert self.images.shape == (self.labels.shape[0], 33, 33)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.images[index], self.labels[index]\n",
        "        image = Image.fromarray(image)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "train_set = MnistRotDataset('train', ToTensor())\n",
        "test_set = MnistRotDataset('test', ToTensor())\n",
        "\n",
        "def train_model(model: torch.nn.Module):\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, batch_size=64)\n",
        "  loss_function = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(30)):\n",
        "    \n",
        "    for i, (x, t) in enumerate(train_loader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.to(device)\n",
        "        t = t.to(device)\n",
        "\n",
        "        y = model(x)\n",
        "\n",
        "        loss = loss_function(y, t)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "    \n",
        "  return model\n",
        "\n",
        "\n",
        "def test_model(model: torch.nn.Module):\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size=64)\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      model.eval()\n",
        "      for i, (x, t) in tqdm.tqdm(enumerate(test_loader)):\n",
        "\n",
        "          x = x.to(device)\n",
        "          t = t.to(device)\n",
        "          \n",
        "          y = model(x)\n",
        "\n",
        "          _, prediction = torch.max(y.data, 1)\n",
        "          total += t.shape[0]\n",
        "          correct += (prediction == t).sum().item()\n",
        "  accuracy = correct/total*100.\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_O2KLRpNo2z"
      },
      "source": [
        "You can now trian and test your neural network.\n",
        "With the default parameters you should achieve an accuracy of roughly **93-94%**.\n",
        "Feel free to adapt the training procedure to improve the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-Ib5ZW2I4V-"
      },
      "outputs": [],
      "source": [
        "model = C4CNN()\n",
        "\n",
        "model = train_model(model)\n",
        "\n",
        "acc = test_model(model)\n",
        "print(f'Test Accuracy: {acc :.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0uK2r0yR5fA"
      },
      "outputs": [],
      "source": [
        "# Let's check if the model is still invariant!\n",
        "\n",
        "x = torch.randn(5, 1, 33, 33)\n",
        "\n",
        "y = net(x)\n",
        "\n",
        "gx = rotate(x, 1)\n",
        "\n",
        "gy = net(gx)\n",
        "\n",
        "assert torch.allclose(y, gy, atol=1e-5, rtol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qzEcXOcOLHL"
      },
      "source": [
        "### 3.3 Implement Equivariant CNN with Isotropic Convolution (5pt)\n",
        "\n",
        "For comparison, implement a similar network by using the `IsotropicConv2d` module you implement before.\n",
        "The model should have the same structure (kernel size, channels, padding, etc..) of the previous model.\n",
        "Except for the last one, in all convolutional layers you should increase by a factor of $4$ the number of output channels since the output of `IsotropicConv2d` is $4$ times smaller than the output of `GroupConv2d`, if the same value for `out_channels` is used.\n",
        "Note that in this model you will use `MaxPool2d` rather than `MaxPool3d`.\n",
        "However, in the last convolutional layer you can keep the same number of channels; we will not need to perform pooling here.\n",
        "\n",
        "Follow the given template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BlXpItbNmNr"
      },
      "outputs": [],
      "source": [
        "class IsotropicCNN(torch.nn.Module):\n",
        "  def __init__(self, n_classes=10):\n",
        "\n",
        "    super(IsotropicCNN, self).__init__()\n",
        "\n",
        "    old_channels = [8, 16, 32, 64, 128]\n",
        "\n",
        "    channels = [4*c for c in old_channels[:-1]] + [old_channels[-1]]\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "  def forward(self, input: torch.Tensor):\n",
        "    \n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATAo2a0GQeDT"
      },
      "outputs": [],
      "source": [
        "# Let's try our model\n",
        "\n",
        "net = IsotropicCNN()\n",
        "\n",
        "x = torch.randn(5, 1, 33, 33)\n",
        "\n",
        "y = net(x)\n",
        "\n",
        "assert y.shape == (5, 10)\n",
        "\n",
        "# Let's check if the model is invariant!\n",
        "\n",
        "gx = rotate(x, 1)\n",
        "\n",
        "gy = net(gx)\n",
        "\n",
        "assert torch.allclose(y, gy, atol=1e-5, rtol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh2l5tC-5JTb"
      },
      "source": [
        "You can now train and test your model.\n",
        "With the default parameters you should achieve an accuracy of roughly **45-50%**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDGfuzpKQpVc"
      },
      "outputs": [],
      "source": [
        "model = IsotropicCNN()\n",
        "\n",
        "model = train_model(model)\n",
        "\n",
        "acc = test_model(model)\n",
        "print(f'Test Accuracy: {acc :.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov2K_hqsR_zR"
      },
      "outputs": [],
      "source": [
        "# Let's check if the model is still invariant!\n",
        "\n",
        "x = torch.randn(5, 1, 33, 33)\n",
        "\n",
        "y = net(x)\n",
        "\n",
        "gx = rotate(x, 1)\n",
        "\n",
        "gy = net(gx)\n",
        "\n",
        "assert torch.allclose(y, gy, atol=1e-5, rtol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x2CCRE1ULib"
      },
      "source": [
        "### 3.4 Exercise: Analyse the results (5pt)\n",
        "Do you note any difference between the performance of the `IsotropicCNN` and the `C4CNN`?\n",
        "What do you think causes this gap? Explain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPU2_zs0Uesq"
      },
      "source": [
        "#### 3.4 Insert Your Solution Here:\n",
        "\n",
        "> YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmuhBQE3QyNZ"
      },
      "source": [
        "### 3.5 Batch Normalization (10 pt)\n",
        "\n",
        "Batch normalization is a common module in most deep neural networks.\n",
        "How can we define BatchNormalization to make it equivariant?\n",
        "\n",
        "Describe how an equivariant batch normalization can be constructed.\n",
        "Then, implement a batch normalization layer.\n",
        "Finally, build a new CNN model similar to `C4CNN` which uses your batchnormalization after each convolution.\n",
        "Train your model and test that it is still equivariant after training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Fx_TMFhRuY"
      },
      "source": [
        "#### 3.5 Insert Your Solution Here:\n",
        "\n",
        "> YOUR ANSWER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj1AYjDKhw4M"
      },
      "outputs": [],
      "source": [
        "class C4CNNWithBatchNorm(torch.nn.Module):\n",
        "  def __init__(self, n_classes=10):\n",
        "\n",
        "    super(C4CNNWithBatchNorm, self).__init__()\n",
        "\n",
        "    channels = [8, 16, 32, 64, 128]\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "  def forward(self, input: torch.Tensor):\n",
        "    \n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "# Let's try our model\n",
        "\n",
        "net = C4CNNWithBatchNorm()\n",
        "\n",
        "x = torch.randn(5, 1, 33, 33)\n",
        "\n",
        "y = net(x)\n",
        "\n",
        "assert y.shape == (5, 10)\n",
        "\n",
        "# Let's check if the model is invariant!\n",
        "\n",
        "gx = rotate(x, 1)\n",
        "\n",
        "gy = net(gx)\n",
        "\n",
        "assert torch.allclose(y, gy, atol=1e-5, rtol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAdSgAb95QWU"
      },
      "source": [
        "You can now train and test your model.\n",
        "With the default parameters you should achieve an accuracy of roughly **95%**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EHe7DkRiI6V"
      },
      "outputs": [],
      "source": [
        "model = C4CNNWithBatchNorm()\n",
        "\n",
        "model = train_model(model)\n",
        "\n",
        "acc = test_model(model)\n",
        "print(f'Test Accuracy: {acc :.3f}')\n",
        "\n",
        "\n",
        "# Let's check if the model is still invariant!\n",
        "x = torch.randn(5, 1, 33, 33)\n",
        "y = net(x)\n",
        "gx = rotate(x, 1)\n",
        "gy = net(gx)\n",
        "assert torch.allclose(y, gy, atol=1e-5, rtol=1e-5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkWHybYLSvO1"
      },
      "source": [
        "# 4 Bonus Question (15 pt)\n",
        "\n",
        "This bonus question is not required to get 100% of the points.\n",
        "Solving this question, however, will give you a few additional points.\n",
        "We recommend solving this question only after you completed the rest of the notebook.\n",
        "\n",
        "So far, we have only considered rotation equivariance.\n",
        "At the beginning of the notebook, we defined the group $D_4$ of rotations and reflections.\n",
        "Together with translations, this forms the group $p4m$.\n",
        "\n",
        "In this exercise, you need to build a `D4CNN` model.\n",
        "This model is similar to `C4CNN` but the group convolutions will also include reflections.\n",
        "This means that, instead of $4$ rotational channels, the features will have $8$ channels (1 for each rotation and reflection).\n",
        "You should first implement the corresponding `D4LiftingConv2d` and `D4GroupConv2d` and then use them to build the `D4CNN`.\n",
        "\n",
        "Finally, test that the model is equivariant to both rotations and translations.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tutorial on Group Convolutional Networks - AMMI Geometric Deep Learning Course",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
